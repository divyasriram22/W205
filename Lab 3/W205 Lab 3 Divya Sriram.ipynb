{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W205 Lab 2 Submissions Divya Sriram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers to Lab 3 Questions (command line prompt input and the resulting outputs are c/p in the second half of this document.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Execution times for Hive, SparkSQL, and SparkSQL on Parquet\n",
    "\n",
    "Hive:  120.034 seconds      \n",
    "SparkSQL: 30.717 seconds\n",
    "SparkSQL on Parquet: 10.761 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Number of jobs launched for Hive and SparkSQL\n",
    "\n",
    "Hive: 2 jobs\n",
    "SparkSQL: 1 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3. Write a query which joins weblogs_parquet to user_info and counts the top 5 locations. List the locations.\n",
    "\n",
    "SELECT user_info.location, \n",
    "COUNT(user_info.location) AS location_count\n",
    "FROM user_info \n",
    "JOIN weblogs_parquet\n",
    "ON user_info.user_id = weblogs_parquet.user_id \n",
    "GROUP BY user_info.location\n",
    "ORDER BY location_count DESC\n",
    "LIMIT 5;\n",
    "\n",
    "\n",
    "La Fayette      49                                                              \n",
    "Leeds\t47\n",
    "Blountsville\t46\n",
    "Hayden\t45\n",
    "Hamilton\t45\n",
    "Time taken: 11.187 seconds, Fetched 5 row(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW IS THE INPUT AND OUTPUT FROM THE COMMAND LINE PROMPTS FOR LAB 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive:\n",
    "    When using hive to count the 50 most frequent user_ids, it launched 2 jobs and took 120.034 seconds.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    SELECT user_id, COUNT(user_id) AS log_count\n",
    "    FROM weblogs_schema GROUP BY user_id\n",
    "    ORDER BY log_count DESC\n",
    "    LIMIT 50;\n",
    "    \n",
    "    --------\n",
    "    \n",
    "Query ID = w205_20170203235555_805d5bb1-8a55-4ab7-b9e3-a5e8d680480b\n",
    "Total jobs = 2\n",
    "Launching Job 1 out of 2\n",
    "Number of reduce tasks not specified. Estimated from input data size: 1\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1486165776661_0003, Tracking URL = http://ip-172-31-6-250.ec2.internal:8088/proxy/application_1486165776661_0003/\n",
    "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1486165776661_0003\n",
    "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
    "2017-02-03 23:55:58,058 Stage-1 map = 0%,  reduce = 0%\n",
    "2017-02-03 23:56:16,045 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.4 sec\n",
    "2017-02-03 23:56:34,388 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.12 sec\n",
    "MapReduce Total cumulative CPU time: 9 seconds 120 msec\n",
    "Ended Job = job_1486165776661_0003\n",
    "Launching Job 2 out of 2\n",
    "Number of reduce tasks determined at compile time: 1\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1486165776661_0004, Tracking URL = http://ip-172-31-6-250.ec2.internal:8088/proxy/application_1486165776661_0004/\n",
    "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1486165776661_0004\n",
    "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
    "2017-02-03 23:56:56,018 Stage-2 map = 0%,  reduce = 0%\n",
    "2017-02-03 23:57:10,362 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.64 sec\n",
    "2017-02-03 23:57:26,442 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.64 sec\n",
    "MapReduce Total cumulative CPU time: 6 seconds 640 msec\n",
    "Ended Job = job_1486165776661_0004\n",
    "MapReduce Jobs Launched: \n",
    "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 9.12 sec   HDFS Read: 5199465 HDFS Write: 867513 SUCCESS\n",
    "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 6.64 sec   HDFS Read: 871961 HDFS Write: 2001 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 15 seconds 760 msec\n",
    "OK\n",
    "RequestVerificationToken_Lw__=2C2DB\t10\n",
    "__RequestVerificationToken_Lw__=3DDC1\t9\n",
    "__RequestVerificationToken_Lw__=A1BC3\t9\n",
    "...(more entries here)\n",
    "__\n",
    "__RequestVerificationToken_Lw__=C22C2\t7\n",
    "Time taken: 120.034 seconds, Fetched: 50 row(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL:\n",
    "    When using SparkSQL to count the 50 most frequent user_ids, it launched only 1 job and took only 30.717 seconds.\n",
    "    \n",
    "    \n",
    "    SELECT\n",
    "    user_id,\n",
    "    COUNT(user_id) AS log_count \n",
    "    FROM weblogs_schema\n",
    "    GROUP BY user_id\n",
    "    ORDER BY log_count DESC\n",
    "    LIMIT 50;\n",
    "\n",
    "    ---------------\n",
    "   \n",
    "__RequestVerificationToken_Lw__=B232C\t8\n",
    "__RequestVerificationToken_Lw__=D1DBD\t8\n",
    "__RequestVerificationToken_Lw__=BD11A\t8\n",
    "__RequestVerificationToken_Lw__=AAABA\t8\n",
    "_...(more entries)\n",
    "__RequestVerificationToken_Lw__=AC3DA\t7\n",
    "Time taken: 30.717 seconds, Fetched 50 row(s)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL on Parquet:\n",
    "    When using SparkSQL to count the 50 most frequent user_ids, it launched only 1 job and took only 10.761 seconds.\n",
    "    \n",
    "    \n",
    "    SELECT user_id, COUNT(user_id) AS log_count\n",
    "    FROM weblogs_parquet GROUP BY user_id\n",
    "    ORDER BY log_count DESC\n",
    "    LIMIT 50;\n",
    "    \n",
    "    -----------\n",
    "\n",
    "__RequestVerificationToken_Lw__=D1DBD\t8\n",
    "__RequestVerificationToken_Lw__=BD11A\t8\n",
    "__RequestVerificationToken_Lw__=DBBC1\t8\n",
    "__RequestVerificationToken_Lw__=33ABD\t8\n",
    "_...(more entries)\n",
    "__RequestVerificationToken_Lw__=AC3DA\t7\n",
    "Time taken: 10.761 seconds, Fetched 50 row(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
