{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W205 Lab 5 Submissions Divya Sriram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBMISSIONS LAST 2 SECTIONS BELOW. \n",
    "The following are notes to myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "IMPORTANT NOTE TO SELF: \n",
    "Everything you do in pyspark is only remembered for that session. Commands run and RDDs created are not saved for the future.\n",
    "\n",
    "(*) pyspark is generally faster than hive at big jobs?\n",
    "(*) but hive is faster than pyspark at little jobs\n",
    "\n",
    "There are 2 ways to create an RDD\n",
    "\n",
    "1. Parallelize an existing collection in your driver program\n",
    "2. Reference a dataset n an external storage system, such as a shared file system, HDFS, HBase, \n",
    "or any data source offering a Hadoop Input Format.\n",
    "\n",
    "\n",
    "(1) Parallelize:\n",
    " \n",
    "(from Lab 4)\n",
    "So far you have used only Python statements and checked that you have a Spark context. You can use the Spark context to create a Spark RDD from Python data using the command parallelize.\n",
    "\n",
    ">>> distData = sc.parallelize(x);\n",
    ">>> print distData;\n",
    "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:391\n",
    "    \n",
    "    \n",
    "(2) Reference a data set\n",
    "Now create an RDD from that file using the following action:\n",
    "\n",
    ">>>crimedata=sc.textFile(\"file:///data/mylab/Crimes_-_2001_to_present.csv\")\n",
    "If you run the AIM, you can copy the data to HDFS and have pyspark get the file from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "    \n",
    "    lab directions say: >>>crimedata=sc.textFile(\"file:///data/mylab/Crimes_-_2001_to_present.csv\")\n",
    "    \n",
    "    specific path I used: \n",
    "        >>> crimedata=sc.textFile(\"file:///home/w205/w205-spring-17-labs-exercises/data/Crimes_-_2001_to_present_data/Crimes_-_2001_to_present_data.csv\")\n",
    "        \n",
    "    \n",
    "        \n",
    "(*) RDDs are immutable. You cannot just go in and remove a record. \n",
    "This is an inherent characteristic of Spark, which allows it to do certain things more efficiently.\n",
    "(*) to remove a header from an RDD, you need to essentially create a new one from the original but without the header\n",
    ">>>noHeaderCrimedata = crimedata.zipWithIndex().filter(lambda (row,index): index > 0).keys()\n",
    "\n",
    "(*) split long lines of data by separating by commas \n",
    ">>> narcoticsCrimeRecords = narcoticsCrimes.map(lambda r : r.split(\",\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Key Values\n",
    "    \n",
    "(*) key value pairs in python are represented as python tuples\n",
    ">>> narcoticsCrimeTuples = narcoticsCrimes.map(lambda x:(x.split(\",\")[0], x))\n",
    "\n",
    "(*) number of tuples is the same as the number of records in the data.\n",
    "\n",
    "(*) the set of lines below creates tuples and observes how they work\n",
    "    \n",
    "    >>> narcoticsCrimeTuples = narcoticsCrimes.map(lambda x:(x.split(\",\")[0], x))\n",
    "    >>> narcoticsCrimeTuples.count()\n",
    "    >>> narcoticsCrimeTuples.first()\n",
    "    >>> firstTuple=narcoticsCrimeTuples.first()\n",
    "    >>> len(firstTuple)\n",
    "    >>> firstTuple[0] --> tuple key\n",
    "    >>> firstTuple[1] --> tuple value\n",
    "    \n",
    "\n",
    "The tuple contains two elements in the array. The first element is the key of the tuple \n",
    "and the second element is value of the tuple.\n",
    "\n",
    "\n",
    "(*) understanding what the map / lambda functions do: \n",
    "http://book.pythontips.com/en/latest/map_filter.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Spark SQL CLI (Beeline client) vs Spark SQL\n",
    "\n",
    "Spark SQL can be used directly from pyspark or a scala shell. But there is also a Spark SQL CLI called the Beeline client. If you use pyspark or use Spark SQL programmatically, you need create a special Spark SQL context. With the Spark SQL CLI, the context is already there for you and you can use SQL commands.\n",
    "\n",
    "\n",
    "(*) Spark SQL is used straight from pyspark (or scala shell) - have to create a context\n",
    "If you use pyspark or use Spark SQL programmatically, you need create a special Spark SQL context.\n",
    "    (*) Step 7 does this\n",
    "    \n",
    "(*) Spark SQL CLI = Beeline client\n",
    "    (*) with this, you can just use SQL commands in the environment as is\n",
    "    (*) to start Beeline, you need explicitly start it with : $spark-sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Spark SQL Table Loaded With Data From a CSV file\n",
    "\n",
    "(*) SQL developers have to decide what types of data will be stored inside each and every table column when creating a SQL table.\n",
    "(*) varchar = tells the table that the character string stored in the column will vary in length\n",
    "\n",
    "(*) You can load files from the local files system or from HDFS. \n",
    "\n",
    "What I learned to do: Spark SQL CLI and how to load data into the empty table. You also practiced some simple SQL commands on the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7:\n",
    "\n",
    "(*) specific path I used to read weblog data into RDD\n",
    ">>> lines = sc.textFile('file:///home/w205/w205-spring-17-labs-exercises/data/weblog_lab.csv')\n",
    "\n",
    "STEPS\n",
    "    (1) read weblog data into RDD\n",
    "    (2) create map of data (to put into table)\n",
    "    (3) create ONE string of names for each column\n",
    "    (4) create ONE array of structfields for the table\n",
    "    (5) put together 3+4 to create a schema object of the names of columns + structfields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission 1:\n",
    "\n",
    "Input:\n",
    ">>> narcoticsCrimeTuples_nonrepeat = narcoticsCrimes.map(lambda x:(x.split(\",\")[0],x.split(\",\")[1:]))\n",
    ">>> narcoticsCrimeTuples_nonrepeat.first()\n",
    "\n",
    "Output:\n",
    "(u'10184515', [u'HY372204', u'08/06/2015 11:55:00 PM', u'033XX W DIVERSEY AVE', u'2027', u'NARCOTICS', u'POSS: CRACK', u'STREET', u'true', u'false', u'1412', u'014', u'35', u'22', u'18', u'1153440', u'1918377', u'2015', u'08/13/2015 12:57:42 PM', u'41.931870591', u'-87.711546895', u'\"(41.931870591', u' -87.711546895)\"'])\n",
    "\n",
    "Note: \n",
    "In comparison to the mapping that we did earlier, the [ ] brackets in the above indicate that we have two distinct values for the key and the value of the tuple. The [] show the sectioning of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission 2:\n",
    "\n",
    "Input:\n",
    ">>> ebayresults.show()    \n",
    ">>> ebayresults = sqlContext.sql(\"select count(*) from Web_Session_Log WHERE REFERERURL = 'http://www.ebay.com'\")\n",
    "\n",
    "Output:\n",
    "\n",
    "+----+\n",
    "| _c0|\n",
    "+----+\n",
    "|3943|\n",
    "+----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
